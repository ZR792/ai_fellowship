# Week 03 â€“ Advanced Prompting: Zero-Shot, Few-Shot, and Chain-of-Thought (CoT)

## ğŸ“Œ Task Overview
The focus of this weekâ€™s assignment was to **experiment with different prompting strategies** and analyze their impact on reasoning accuracy and clarity.  
The three prompting methods explored were:
- **Zero-Shot** â†’ Direct question, no examples.
- **Few-Shot** â†’ Provide 2â€“3 examples before asking the main question.
- **Chain-of-Thought (CoT)** â†’ Explicitly guide the model to think step by step.

The goal was to compare how each approach affects model reasoning when solving tasks from the given datasets:
- `math-problems.json`
- `logic-puzzles.json`
- `reasoning-tasks.json`

---

## ğŸ“‚ Project Structure
``` 
Week_03_AI_Fellowship/
â”‚â”€â”€ README.md â† This file
â”‚â”€â”€ week_03_report.md â† Final detailed report with prompts, outputs, and evaluation
â”‚â”€â”€ evaluation_table.md â† Rubric-based scoring table
â”‚
â”œâ”€â”€ prompts/ â† Prompt templates used
â”‚ â”œâ”€â”€ math_Prompt.md
â”‚ â”œâ”€â”€ logic_Puzzle_propmt.md
â”‚ â”œâ”€â”€ reasoning_prompt.md
| 
â””â”€â”€ outputs/ â† Raw model outputs collected
â”œâ”€â”€ math_outputs.md
â”œâ”€â”€ logic_outputs.md
â”œâ”€â”€ reasoning_outputs.md
``` 

---

## ğŸ“ What I Did
1. **Selected three tasks** â€“ one math problem, one logic puzzle, and one reasoning question from the datasets.  
2. **Designed prompts** for each problem using:  
   - Zero-Shot (direct question)  
   - Few-Shot (2 solved examples + main problem)  
   - Chain-of-Thought (step-by-step reasoning instruction)  
3. **Collected outputs** â€“ recorded the exact answers generated by the model.  
4. **Evaluated responses** using the **Reasoning Evaluation Rubric** (Correctness, Clarity, Completeness, Conciseness).  
5. **Compared results** across all three strategies in `week_03_report.md`.  

---

## ğŸ“Š Key Insights
- **Zero-Shot** â†’ Works fine for simple factual questions but often lacks reasoning detail.  
- **Few-Shot** â†’ Improves accuracy by showing examples; the model mimics reasoning patterns.  
- **Chain-of-Thought (CoT)** â†’ Provided the **most reliable and complete answers**, especially for multi-step reasoning tasks.  

---

## ğŸ’¡ Learning Outcomes
- I learned how **prompt design directly influences model performance**.  
- Saw how adding examples (Few-Shot) or reasoning instructions (CoT) helps the model avoid mistakes.  
- Understood the importance of **structured evaluation** using a rubric rather than just checking if the answer is right or wrong.  
- Gained practical experience in setting up prompts, collecting outputs, and critically comparing results.  

---

## âœ… Conclusion
This weekâ€™s assignment deepened my understanding of **advanced prompting techniques**.  
I discovered that while Zero-Shot can be fast, Few-Shot and especially CoT make the model much more **accurate, transparent, and reliable** for reasoning-based tasks.  
These skills are foundational for building better AI workflows, as effective prompting is the key to unlocking strong performance from large language models.  
