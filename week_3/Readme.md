# Week 03 – Advanced Prompting: Zero-Shot, Few-Shot, and Chain-of-Thought (CoT)

## 📌 Task Overview
The focus of this week’s assignment was to **experiment with different prompting strategies** and analyze their impact on reasoning accuracy and clarity.  
The three prompting methods explored were:
- **Zero-Shot** → Direct question, no examples.
- **Few-Shot** → Provide 2–3 examples before asking the main question.
- **Chain-of-Thought (CoT)** → Explicitly guide the model to think step by step.

The goal was to compare how each approach affects model reasoning when solving tasks from the given datasets:
- `math-problems.json`
- `logic-puzzles.json`
- `reasoning-tasks.json`

---

## 📂 Project Structure
``` 
Week_03_AI_Fellowship/
│── README.md ← This file
│── week_03_report.md ← Final detailed report with prompts, outputs, and evaluation
│── evaluation_table.md ← Rubric-based scoring table
│
├── prompts/ ← Prompt templates used
│ ├── math_Prompt.md
│ ├── logic_Puzzle_propmt.md
│ ├── reasoning_prompt.md
| 
└── outputs/ ← Raw model outputs collected
├── math_outputs.md
├── logic_outputs.md
├── reasoning_outputs.md
``` 

---

## 📝 What I Did
1. **Selected three tasks** – one math problem, one logic puzzle, and one reasoning question from the datasets.  
2. **Designed prompts** for each problem using:  
   - Zero-Shot (direct question)  
   - Few-Shot (2 solved examples + main problem)  
   - Chain-of-Thought (step-by-step reasoning instruction)  
3. **Collected outputs** – recorded the exact answers generated by the model.  
4. **Evaluated responses** using the **Reasoning Evaluation Rubric** (Correctness, Clarity, Completeness, Conciseness).  
5. **Compared results** across all three strategies in `week_03_report.md`.  

---

## 📊 Key Insights
- **Zero-Shot** → Works fine for simple factual questions but often lacks reasoning detail.  
- **Few-Shot** → Improves accuracy by showing examples; the model mimics reasoning patterns.  
- **Chain-of-Thought (CoT)** → Provided the **most reliable and complete answers**, especially for multi-step reasoning tasks.  

---

## 💡 Learning Outcomes
- I learned how **prompt design directly influences model performance**.  
- Saw how adding examples (Few-Shot) or reasoning instructions (CoT) helps the model avoid mistakes.  
- Understood the importance of **structured evaluation** using a rubric rather than just checking if the answer is right or wrong.  
- Gained practical experience in setting up prompts, collecting outputs, and critically comparing results.  

---

## ✅ Conclusion
This week’s assignment deepened my understanding of **advanced prompting techniques**.  
I discovered that while Zero-Shot can be fast, Few-Shot and especially CoT make the model much more **accurate, transparent, and reliable** for reasoning-based tasks.  
These skills are foundational for building better AI workflows, as effective prompting is the key to unlocking strong performance from large language models.  
