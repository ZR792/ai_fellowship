## Photo Analysis

**Image file:** photo.jpg

**Caption / Description:**

a square and a square are shown in the same color



## Document OCR

**Image file:** document.png

**OCR text (first 800 chars):**

```
Til: Sample Research Report

‘Author Test Author

Abstract Thisisa synthetic abstract used to test OCRand documentextraction.

We demonstrate extraction of headings paragraphs and small tables.

4.ntroduetion

Thisdocumentiscreated for the multimadalassignment

2. Metheds

‘We used sample data and synthetic text

Table 1 Results

Metricvalue

‘Accuracy 0.92

Fio89

```



## Chart Analysis

**Image file:** chart.png

**Chart analysis (vision/model):**

The OCR is a symbiotic tool for analyzing the market and determining the market's market value.



## Long-context RAG demo

Extracted 35539 characters from long_text.pdf

Preview:

```
Providedproperattributionisprovided,Googleherebygrantspermissionto
reproducethetablesandfiguresinthispapersolelyforuseinjournalisticor
scholarlyworks.
Attention Is All You Need
AshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗
GoogleBrain GoogleBrain GoogleResearch GoogleResearch
avaswani@google.com noam@google.com nikip@google.com usz@google.com
LlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗
GoogleResearch UniversityofToronto GoogleBrain
llion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com
IlliaPolosukhin∗ ‡
illia.polosukhin@gmail.com
Abstract
Thedominantsequencetransductionmodelsarebasedoncomplexrecurrentor
convolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new s
```

Chunks created: 14

FAISS index built (local embeddings).

Sample retrieval previews (top hits):

- rep tcefrep , , tub tub sti sti noitacilppa noitacilppa dluohs dluohs eb eb tsuj tsuj - - siht siht si si tahw tahw ew ew era era gnissim gnissim , , ni ni ym ym noinipo noinipo . . >SOE< >SOE< >dap< >dap< Figure5: Manyoftheattentionheadsexhibitbehaviourthatseemsrelatedtothestructureofthe sentence. Wegivetwosuchexamplesabove,fromtwodifferentheadsfromtheencoderself-attention atlayer5of6. Theheadscl

- izationtricksforLSTMnetworks. arXivpreprint arXiv:1703.10722,2017. [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130,2017. [23] Minh-ThangLuong,QuocV.Le,IlyaSutskever,OriolVinyals,andLukaszKaiser. Multi-task sequencetosequencelearning. arXivpreprintarXiv:1511.0

- e with sentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece [38]andbyte-pair[31]representations. Toimprovecomputationalperformancefortasksinvolving verylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin theinputsequencecenteredaroundtherespectiveoutputposition. Thiswouldincreasethemaximum pathlengthtoO(n/r). Weplantoinvestigate

- on consists of several attentionlayersrunninginparallel. ofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe querywiththecorrespondingkey. 3.2.1 ScaledDot-ProductAttention Wecallourparticularattention"ScaledDot-ProductAttention"(Figure2). Theinputconsistsof queriesandkeysofdimensiond k ,a√ndvaluesofdimensiond v . Wecomputethedotproductsofthe querywithallkeys,divide

RAG answer (sample):

Not enough information
