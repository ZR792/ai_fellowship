[
  {
    "question": "Summarize the paper's main contribution in two sentences.",
    "rag_answer": "allyfactor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a new state of the art in training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. In this work we propose the Transformer, a model architecture eschewing continuity and instead relying entirely on an attention mechanism to draw global independence between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twenty hours on eight P100 GPUs. 2",
    "rag_hits": [
      {
        "id": 10,
        "score": 0.2809310853481293,
        "chunk": "entive sentence embedding. arXiv preprint\narXiv:1703.03130,2017.\n[23] Minh-ThangLuong,QuocV.Le,IlyaSutskever,OriolVinyals,andLukaszKaiser. Multi-task\nsequencetosequencelearning. arXivpreprintarXiv:1511.06114,2015.\n[24] Minh-ThangLuong,HieuPham,andChristopherDManning. Effectiveapproachestoattention-\nbasedneuralmachinetranslation. arXivpreprintarXiv:1508.04025,2015.\n11\n\n[PAGE 12]\n[25] MitchellPMarcus,MaryAnnMarcinkiewicz,andBeatriceSantorini.Buildingalargeannotated\ncorpusofenglish: Thepenntreebank. Computationallinguistics,19(2):313–330,1993.\n[26] DavidMcClosky,EugeneCharniak,andMarkJohnson. Effectiveself-trainingforparsing. In\nProceedingsoftheHumanLanguageTechnologyConferenceoftheNAACL,MainConference,\npages152–159.ACL,June2006.\n[27] AnkurParikh,OscarTäckström,DipanjanDas,andJakobUszkoreit. Adecomposableattention\nmodel. InEmpiricalMethodsinNaturalLanguageProcessing,2016.\n[28] RomainPaulus,CaimingXiong,andRichardSocher. Adeepreinforcedmodelforabstractive\nsummarization. arXivpreprintarXiv:1705.04304,2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputationalLinguisticsand44thAnnualMeetingoftheACL,pages433–440.ACL,July\n2006.\n[30] OfirPressandLiorWolf. Usingtheoutputembeddingtoimprovelanguagemodels. arXiv\npreprintarXiv:1608.05859,2016.\n[31] RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrarewords\nwithsubwordunits. arXivpreprintarXiv:1508.07909,2015.\n[32] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,\nandJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-experts\nlayer. arXivpreprintarXiv:1701.06538,2017.\n[33] NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi-\nnov. Dropout: asimplewaytopreventneuralnetworksfromoverfitting. JournalofMachine\nLearningResearch,15(1):1929–1958,2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. InC.Cortes, N.D.Lawrence, D.D.Lee, M.Sugiyama, andR.Garnett, editors,\nAdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates,\nInc.,2015.\n[35] IlyaSutskever,OriolVinyals,andQuocVVLe. Sequencetosequencelearningwithneural\nnetworks. InAdvancesinNeuralInformationProcessingSystems,pages3104–3112,2014.\n[36] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna.\nRethinkingtheinceptionarchitectureforcomputervision. CoRR,abs/1512.00567,2015.\n[37] Vinyals&Kaiser, Koo, Petrov, Sutskever, andHinton. Grammarasaforeignlanguage. In\nAdvancesinNeuralInformationProcessingSystems,2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal. Google’sneuralmachine\ntranslationsystem: Bridgingthegapbetweenhumanandmachinetranslation. arXivpreprint\narXiv:1609.08144,2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep r"
      },
      {
        "id": 11,
        "score": 0.22482465207576752,
        "chunk": "ecurrent models with\nfast-forwardconnectionsforneuralmachinetranslation. CoRR,abs/1606.04199,2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduceconstituentparsing. InProceedingsofthe51stAnnualMeetingoftheACL(Volume\n1: LongPapers),pages434–443.ACL,August2013.\n12\n\n[PAGE 13]\nInput-Input Layer5\nAttentionVisualizations\ntI\ntI\nsi\nsi\nni\nni\nsiht\nsiht\ntirips\ntirips\ntaht\ntaht\na\na\nytirojam\nytirojam\nfo\nfo\nnaciremA\nnaciremA\nstnemnrevog\nstnemnrevog\nevah\nevah\ndessap\ndessap\nwen\nwen\nswal\nswal\necnis\necnis\n9002\n9002\ngnikam\ngnikam\neht\neht\nnoitartsiger\nnoitartsiger\nro\nro\ngnitov\ngnitov\nssecorp\nssecorp\nerom\nerom\ntluciffid\ntluciffid\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoderself-attentioninlayer5of6. Manyoftheattentionheadsattendtoadistantdependencyof\ntheverb‘making’,completingthephrase‘making...moredifficult’. Attentionshereshownonlyfor\ntheword‘making’. Differentcolorsrepresentdifferentheads. Bestviewedincolor.\n13\n\n[PAGE 14]\nInput-Input Layer5\nehT\nehT\nwaL\nwaL\nlliw\nlliw\nreven\nreven\neb\neb\ntcefrep\ntcefrep\n,\n,\ntub\ntub\nsti\nsti\nnoitacilppa\nnoitacilppa\ndluohs\ndluohs\neb\neb\ntsuj\ntsuj\n-\n-\nsiht\nsiht\nsi\nsi\ntahw\ntahw\new\new\nera\nera\ngnissim\ngnissim\n,\n,\nni\nni\nym\nym\nnoinipo\nnoinipo\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\nInput-Input Layer5\nehT\nehT\nwaL\nwaL\nlliw\nlliw\nreven\nreven\neb\neb\ntcefrep\ntcefrep\n,\n,\ntub\ntub\nsti\nsti\nnoitacilppa\nnoitacilppa\ndluohs\ndluohs\neb\neb\ntsuj\ntsuj\n-\n-\nsiht\nsiht\nsi\nsi\ntahw\ntahw\new\new\nera\nera\ngnissim\ngnissim\n,\n,\nni\nni\nym\nym\nnoinipo\nnoinipo\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\nFigure4: Twoattentionheads,alsoinlayer5of6,apparentlyinvolvedinanaphoraresolution. Top:\nFullattentionsforhead5. Bottom: Isolatedattentionsfromjusttheword‘its’forattentionheads5\nand6. Notethattheattentionsareverysharpforthisword.\n14\n\n[PAGE 15]\nInput-Input Layer5\nehT\nehT\nwaL\nwaL\nlliw\nlliw\nreven\nreven\neb\neb\ntcefrep\ntcefrep\n,\n,\ntub\ntub\nsti\nsti\nnoitacilppa\nnoitacilppa\ndluohs\ndluohs\neb\neb\ntsuj\ntsuj\n-\n-\nsiht\nsiht\nsi\nsi\ntahw\ntahw\new\new\nera\nera\ngnissim\ngnissim\n,\n,\nni\nni\nym\nym\nnoinipo\nnoinipo\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\nInput-Input Layer5\nehT\nehT\nwaL\nwaL\nlliw\nlliw\nreven\nreven\neb\neb\ntcefrep\ntcefrep\n,\n,\ntub\ntub\nsti\nsti\nnoitacilppa\nnoitacilppa\ndluohs\ndluohs\neb\neb\ntsuj\ntsuj\n-\n-\nsiht\nsiht\nsi\nsi\ntahw\ntahw\new\new\nera\nera\ngnissim\ngnissim\n,\n,\nni\nni\nym\nym\nnoinipo\nnoinipo\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\nFigure5: Manyoftheattentionheadsexhibitbehaviourthatseemsrelatedtothestructureofthe\nsentence. Wegivetwosuchexamplesabove,fromtwodifferentheadsfromtheencoderself-attention\natlayer5of6. Theheadsclearlylearnedtoperformdifferenttasks.\n15"
      },
      {
        "id": 1,
        "score": 0.1740017831325531,
        "chunk": "allyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstru"
      }
    ],
    "non_rag_answer": "The paper's main contribution is to the development of the 'strange' strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange strange s"
  },
  {
    "question": "What dataset(s) are used and what preprocessing steps are mentioned?",
    "rag_answer": "[chunk_id:1] glish-French dataset consisting of 36Msentences and split tokens into a 32000 word-piece vocabulary[38].Sentence pairs were tested together by approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Summary --- [chunk_id:1] allyfactor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time,they generate a sensequence of hidden statesh ,a function of the previous hidden statesh and the input for positiont. This inherently t t1 sequential nature prevents parallelizationwithin training examples, which becomes critical at long sequence lengths,asmemory constraints of the neural network --- [chunk_id:1] allyfactor computation along the symbol positions of the input",
    "rag_hits": [
      {
        "id": 6,
        "score": 0.2819971442222595,
        "chunk": "glish-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece\nvocabulary[38].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining\nbatchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000\ntargettokens.\n5.2 HardwareandSchedule\nWetrainedourmodelsononemachinewith8NVIDIAP100GPUs. Forourbasemodelsusing\nthehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds. We\ntrainedthebasemodelsforatotalof100,000stepsor12hours. Forourbigmodels,(describedonthe\nbottomlineoftable3),steptimewas1.0seconds. Thebigmodelsweretrainedfor300,000steps\n(3.5days).\n5.3 Optimizer\nWeusedtheAdamoptimizer[20]withβ =0.9,β =0.98andϵ=10−9. Wevariedthelearning\n1 2\nrateoverthecourseoftraining,accordingtotheformula:\nlrate=d−0.5 ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\nmodel\nThiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps,\nanddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber. Weused\nwarmup_steps=4000.\n5.4 Regularization\nWeemploythreetypesofregularizationduringtraining:\n7\n\n[PAGE 8]\nTable2: TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe\nEnglish-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost.\nBLEU TrainingCost(FLOPs)\nModel\nEN-DE EN-FR EN-DE EN-FR\nByteNet[18] 23.75\nDeep-Att+PosUnk[39] 39.2 1.0·1020\nGNMT+RL[38] 24.6 39.92 2.3·1019 1.4·1020\nConvS2S[9] 25.16 40.46 9.6·1018 1.5·1020\nMoE[32] 26.03 40.56 2.0·1019 1.2·1020\nDeep-Att+PosUnkEnsemble[39] 40.4 8.0·1020\nGNMT+RLEnsemble[38] 26.30 41.16 1.8·1020 1.1·1021\nConvS2SEnsemble[9] 26.36 41.29 7.7·1019 1.2·1021\nTransformer(basemodel) 27.3 38.1 3.3·1018\nTransformer(big) 28.4 41.8 2.3·1019\nResidualDropout Weapplydropout[33]totheoutputofeachsub-layer,beforeitisaddedtothe\nsub-layerinputandnormalized. Inaddition,weapplydropouttothesumsoftheembeddingsandthe\npositionalencodingsinboththeencoderanddecoderstacks. Forthebasemodel,weusearateof\nP =0.1.\ndrop\nLabelSmoothing Duringtraining,weemployedlabelsmoothingofvalueϵ = 0.1[36]. This\nls\nhurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore.\n6 Results\n6.1 MachineTranslation\nOntheWMT2014English-to-Germantranslationtask,thebigtransformermodel(Transformer(big)\ninTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0\nBLEU,establishinganewstate-of-the-artBLEUscoreof28.4. Theconfigurationofthismodelis\nlistedinthebottomlineofTable3. Trainingtook3.5dayson8P100GPUs. Evenourbasemodel\nsurpassesallpreviouslypublishedmodelsandensembles,atafractionofthetrainingcostofanyof\nthecompetitivemodels.\nOntheWMT2014English-to-Frenchtranslationtask,ourbigmodelachievesaBLEUscoreof41.0,\noutperformingallofthepreviouslypublishedsinglemodels,atlessthan1/4thetrainingcostofthe\npreviousstate-of-the-artmodel. TheTransformer(big)modeltrainedforEnglish-to-Frenchused\ndropoutrateP =0.1,insteadof0.3.\ndrop\nForthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,which\nwere"
      },
      {
        "id": 1,
        "score": 0.2593599855899811,
        "chunk": "allyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstru"
      },
      {
        "id": 7,
        "score": 0.24156594276428223,
        "chunk": "writtenat10-minuteintervals. Forthebigmodels,weaveragedthelast20checkpoints. We\nusedbeamsearchwithabeamsizeof4andlengthpenaltyα = 0.6[38]. Thesehyperparameters\nwerechosenafterexperimentationonthedevelopmentset. Wesetthemaximumoutputlengthduring\ninferencetoinputlength+50,butterminateearlywhenpossible[38].\nTable2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel\narchitecturesfromtheliterature. Weestimatethenumberoffloatingpointoperationsusedtotraina\nmodelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained\nsingle-precisionfloating-pointcapacityofeachGPU5.\n6.2 ModelVariations\nToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel\nindifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe\n5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively.\n8\n\n[PAGE 9]\nTable3: VariationsontheTransformerarchitecture. Unlistedvaluesareidenticaltothoseofthebase\nmodel. AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013. Listed\nperplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto\nper-wordperplexities.\ntrain PPL BLEU params\nN d d h d d P ϵ\nmodel ff k v drop ls steps (dev) (dev) ×106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n(A)\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n16 5.16 25.1 58\n(B)\n32 5.01 25.4 60\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n(C) 256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n0.0 5.77 24.6\n0.2 4.95 25.5\n(D)\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positionalembeddinginsteadofsinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopmentset,newstest2013. Weusedbeamsearchasdescribedintheprevioussection,butno\ncheckpointaveraging. WepresenttheseresultsinTable3.\nInTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads.\nInTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality. This\nk\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunctionthandotproductmaybebeneficial. Wefurtherobserveinrows(C)and(D)that,asexpected,\nbiggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceour\nsinusoidalpositionalencodingwithlearnedpositionalembeddings[9],andobservenearlyidentical\nresultstothebasemodel.\n6.3 EnglishConstituencyParsing\nToevaluateiftheTransformercangeneralizetoothertasksweperformedexperimentsonEnglish\nconstituencyparsing. Thistaskpresentsspecificchallenges: theoutputissubjecttostrongstructural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodelshavenotbeenabletoattainstate-of-the-artresultsinsmall-dataregimes[37].\nWetraineda4-layertransformerwithd =1024ontheWallS"
      }
    ],
    "non_rag_answer": "a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a) a"
  },
  {
    "question": "Describe the model/architecture and key hyperparameters.",
    "rag_answer": "[chunk_id:1] allyfactor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a free sequence of hidden statesh , as a function of the previous hidden stateh and the input for positiont. This inherently t t1 sequential nature prevents parallelization with in training examples, which becomes critical at long sequence lengths, as memory constraints limit bidding across examples. The fundamental constraint of parallel computation also forms the foundation of the Extended Neural GPU [16], ByteNet[18] and ConvS2S[9],all of which use convolutional neural networks as basic building block, computing hidden representations in par",
    "rag_hits": [
      {
        "id": 5,
        "score": 0.3925396800041199,
        "chunk": " another sequence of equal length (z ,...,z ), with x ,z ∈ Rd, such as a hidden\n1 n 1 n i i\nlayerinatypicalsequencetransductionencoderordecoder. Motivatingouruseofself-attentionwe\nconsiderthreedesiderata.\nOneisthetotalcomputationalcomplexityperlayer. Anotheristheamountofcomputationthatcan\nbeparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired.\nThethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork. Learninglong-range\ndependenciesisakeychallengeinmanysequencetransductiontasks. Onekeyfactoraffectingthe\nabilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto\ntraverseinthenetwork. Theshorterthesepathsbetweenanycombinationofpositionsintheinput\nandoutputsequences,theeasieritistolearnlong-rangedependencies[12]. Hencewealsocompare\nthemaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe\ndifferentlayertypes.\nAsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence\n6\n\n[PAGE 7]\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece\n[38]andbyte-pair[31]representations. Toimprovecomputationalperformancefortasksinvolving\nverylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin\ntheinputsequencecenteredaroundtherespectiveoutputposition. Thiswouldincreasethemaximum\npathlengthtoO(n/r). Weplantoinvestigatethisapproachfurtherinfuturework.\nAsingleconvolutionallayerwithkernelwidthk <ndoesnotconnectallpairsofinputandoutput\npositions. DoingsorequiresastackofO(n/k)convolutionallayersinthecaseofcontiguouskernels,\norO(log (n))inthecaseofdilatedconvolutions[18], increasingthelengthofthelongestpaths\nk\nbetweenanytwopositionsinthenetwork. Convolutionallayersaregenerallymoreexpensivethan\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\nconsiderably, toO(k·n·d+n·d2). Evenwithk = n, however, thecomplexityofaseparable\nconvolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer,\ntheapproachwetakeinourmodel.\nAssidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions\nfromourmodelsandpresentanddiscussexamplesintheappendix. Notonlydoindividualattention\nheadsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic\nandsemanticstructureofthesentences.\n5 Training\nThissectiondescribesthetrainingregimeforourmodels.\n5.1 TrainingDataandBatching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentencepairs. Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource-\ntargetvocabularyofabout37000tokens. ForEnglish-French,weusedthesignificantlylargerWMT\n2014En"
      },
      {
        "id": 7,
        "score": 0.29075080156326294,
        "chunk": "writtenat10-minuteintervals. Forthebigmodels,weaveragedthelast20checkpoints. We\nusedbeamsearchwithabeamsizeof4andlengthpenaltyα = 0.6[38]. Thesehyperparameters\nwerechosenafterexperimentationonthedevelopmentset. Wesetthemaximumoutputlengthduring\ninferencetoinputlength+50,butterminateearlywhenpossible[38].\nTable2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel\narchitecturesfromtheliterature. Weestimatethenumberoffloatingpointoperationsusedtotraina\nmodelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained\nsingle-precisionfloating-pointcapacityofeachGPU5.\n6.2 ModelVariations\nToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel\nindifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe\n5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively.\n8\n\n[PAGE 9]\nTable3: VariationsontheTransformerarchitecture. Unlistedvaluesareidenticaltothoseofthebase\nmodel. AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013. Listed\nperplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto\nper-wordperplexities.\ntrain PPL BLEU params\nN d d h d d P ϵ\nmodel ff k v drop ls steps (dev) (dev) ×106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n(A)\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n16 5.16 25.1 58\n(B)\n32 5.01 25.4 60\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n(C) 256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n0.0 5.77 24.6\n0.2 4.95 25.5\n(D)\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positionalembeddinginsteadofsinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopmentset,newstest2013. Weusedbeamsearchasdescribedintheprevioussection,butno\ncheckpointaveraging. WepresenttheseresultsinTable3.\nInTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads.\nInTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality. This\nk\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunctionthandotproductmaybebeneficial. Wefurtherobserveinrows(C)and(D)that,asexpected,\nbiggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceour\nsinusoidalpositionalencodingwithlearnedpositionalembeddings[9],andobservenearlyidentical\nresultstothebasemodel.\n6.3 EnglishConstituencyParsing\nToevaluateiftheTransformercangeneralizetoothertasksweperformedexperimentsonEnglish\nconstituencyparsing. Thistaskpresentsspecificchallenges: theoutputissubjecttostrongstructural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodelshavenotbeenabletoattainstate-of-the-artresultsinsmall-dataregimes[37].\nWetraineda4-layertransformerwithd =1024ontheWallS"
      },
      {
        "id": 1,
        "score": 0.28781867027282715,
        "chunk": "allyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstru"
      }
    ],
    "non_rag_answer": "Describe the model/architecture and key hyperparameters."
  },
  {
    "question": "What are the main experimental results and metrics reported?",
    "rag_answer": "[chunk_id:1] allyfactor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a free sequence of hidden statesh , as a function of the previous hidden stateh and the input for positiont. This inherently t t1 sequential nature prevents parallelization with in training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. In this work we propose the Transformer, a model architecture eschewing continuity and instead relying entirely on an attention mechanism to draw global independence between input and output sequences [2,19] --- [chunk_id:1] allyfactor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a free sequence of hidden statesh , as a function",
    "rag_hits": [
      {
        "id": 7,
        "score": 0.26808276772499084,
        "chunk": "writtenat10-minuteintervals. Forthebigmodels,weaveragedthelast20checkpoints. We\nusedbeamsearchwithabeamsizeof4andlengthpenaltyα = 0.6[38]. Thesehyperparameters\nwerechosenafterexperimentationonthedevelopmentset. Wesetthemaximumoutputlengthduring\ninferencetoinputlength+50,butterminateearlywhenpossible[38].\nTable2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel\narchitecturesfromtheliterature. Weestimatethenumberoffloatingpointoperationsusedtotraina\nmodelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained\nsingle-precisionfloating-pointcapacityofeachGPU5.\n6.2 ModelVariations\nToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel\nindifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe\n5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively.\n8\n\n[PAGE 9]\nTable3: VariationsontheTransformerarchitecture. Unlistedvaluesareidenticaltothoseofthebase\nmodel. AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013. Listed\nperplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto\nper-wordperplexities.\ntrain PPL BLEU params\nN d d h d d P ϵ\nmodel ff k v drop ls steps (dev) (dev) ×106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n(A)\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n16 5.16 25.1 58\n(B)\n32 5.01 25.4 60\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n(C) 256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n0.0 5.77 24.6\n0.2 4.95 25.5\n(D)\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positionalembeddinginsteadofsinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopmentset,newstest2013. Weusedbeamsearchasdescribedintheprevioussection,butno\ncheckpointaveraging. WepresenttheseresultsinTable3.\nInTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads.\nInTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality. This\nk\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunctionthandotproductmaybebeneficial. Wefurtherobserveinrows(C)and(D)that,asexpected,\nbiggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceour\nsinusoidalpositionalencodingwithlearnedpositionalembeddings[9],andobservenearlyidentical\nresultstothebasemodel.\n6.3 EnglishConstituencyParsing\nToevaluateiftheTransformercangeneralizetoothertasksweperformedexperimentsonEnglish\nconstituencyparsing. Thistaskpresentsspecificchallenges: theoutputissubjecttostrongstructural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodelshavenotbeenabletoattainstate-of-the-artresultsinsmall-dataregimes[37].\nWetraineda4-layertransformerwithd =1024ontheWallS"
      },
      {
        "id": 11,
        "score": 0.1885935366153717,
        "chunk": "ecurrent models with\nfast-forwardconnectionsforneuralmachinetranslation. CoRR,abs/1606.04199,2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduceconstituentparsing. InProceedingsofthe51stAnnualMeetingoftheACL(Volume\n1: LongPapers),pages434–443.ACL,August2013.\n12\n\n[PAGE 13]\nInput-Input Layer5\nAttentionVisualizations\ntI\ntI\nsi\nsi\nni\nni\nsiht\nsiht\ntirips\ntirips\ntaht\ntaht\na\na\nytirojam\nytirojam\nfo\nfo\nnaciremA\nnaciremA\nstnemnrevog\nstnemnrevog\nevah\nevah\ndessap\ndessap\nwen\nwen\nswal\nswal\necnis\necnis\n9002\n9002\ngnikam\ngnikam\neht\neht\nnoitartsiger\nnoitartsiger\nro\nro\ngnitov\ngnitov\nssecorp\nssecorp\nerom\nerom\ntluciffid\ntluciffid\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\n>dap<\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoderself-attentioninlayer5of6. Manyoftheattentionheadsattendtoadistantdependencyof\ntheverb‘making’,completingthephrase‘making...moredifficult’. Attentionshereshownonlyfor\ntheword‘making’. Differentcolorsrepresentdifferentheads. Bestviewedincolor.\n13\n\n[PAGE 14]\nInput-Input Layer5\nehT\nehT\nwaL\nwaL\nlliw\nlliw\nreven\nreven\neb\neb\ntcefrep\ntcefrep\n,\n,\ntub\ntub\nsti\nsti\nnoitacilppa\nnoitacilppa\ndluohs\ndluohs\neb\neb\ntsuj\ntsuj\n-\n-\nsiht\nsiht\nsi\nsi\ntahw\ntahw\new\new\nera\nera\ngnissim\ngnissim\n,\n,\nni\nni\nym\nym\nnoinipo\nnoinipo\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\nInput-Input Layer5\nehT\nehT\nwaL\nwaL\nlliw\nlliw\nreven\nreven\neb\neb\ntcefrep\ntcefrep\n,\n,\ntub\ntub\nsti\nsti\nnoitacilppa\nnoitacilppa\ndluohs\ndluohs\neb\neb\ntsuj\ntsuj\n-\n-\nsiht\nsiht\nsi\nsi\ntahw\ntahw\new\new\nera\nera\ngnissim\ngnissim\n,\n,\nni\nni\nym\nym\nnoinipo\nnoinipo\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\nFigure4: Twoattentionheads,alsoinlayer5of6,apparentlyinvolvedinanaphoraresolution. Top:\nFullattentionsforhead5. Bottom: Isolatedattentionsfromjusttheword‘its’forattentionheads5\nand6. Notethattheattentionsareverysharpforthisword.\n14\n\n[PAGE 15]\nInput-Input Layer5\nehT\nehT\nwaL\nwaL\nlliw\nlliw\nreven\nreven\neb\neb\ntcefrep\ntcefrep\n,\n,\ntub\ntub\nsti\nsti\nnoitacilppa\nnoitacilppa\ndluohs\ndluohs\neb\neb\ntsuj\ntsuj\n-\n-\nsiht\nsiht\nsi\nsi\ntahw\ntahw\new\new\nera\nera\ngnissim\ngnissim\n,\n,\nni\nni\nym\nym\nnoinipo\nnoinipo\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\nInput-Input Layer5\nehT\nehT\nwaL\nwaL\nlliw\nlliw\nreven\nreven\neb\neb\ntcefrep\ntcefrep\n,\n,\ntub\ntub\nsti\nsti\nnoitacilppa\nnoitacilppa\ndluohs\ndluohs\neb\neb\ntsuj\ntsuj\n-\n-\nsiht\nsiht\nsi\nsi\ntahw\ntahw\new\new\nera\nera\ngnissim\ngnissim\n,\n,\nni\nni\nym\nym\nnoinipo\nnoinipo\n.\n.\n>SOE<\n>SOE<\n>dap<\n>dap<\nFigure5: Manyoftheattentionheadsexhibitbehaviourthatseemsrelatedtothestructureofthe\nsentence. Wegivetwosuchexamplesabove,fromtwodifferentheadsfromtheencoderself-attention\natlayer5of6. Theheadsclearlylearnedtoperformdifferenttasks.\n15"
      },
      {
        "id": 1,
        "score": 0.17902538180351257,
        "chunk": "allyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstru"
      }
    ],
    "non_rag_answer": "Statistically significant"
  },
  {
    "question": "What limitations or future work does the paper mention?",
    "rag_answer": "[chunk_id:1] allyfactor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a free sequence of hidden statesh , as a function of the previous hidden stateh and the input for positiont. This inherently t t1 sequential nature prevents parallelization with in training examples, which becomes critical at long sequence lengths, as memory constraints limitbatching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32]",
    "rag_hits": [
      {
        "id": 10,
        "score": 0.23537664115428925,
        "chunk": "entive sentence embedding. arXiv preprint\narXiv:1703.03130,2017.\n[23] Minh-ThangLuong,QuocV.Le,IlyaSutskever,OriolVinyals,andLukaszKaiser. Multi-task\nsequencetosequencelearning. arXivpreprintarXiv:1511.06114,2015.\n[24] Minh-ThangLuong,HieuPham,andChristopherDManning. Effectiveapproachestoattention-\nbasedneuralmachinetranslation. arXivpreprintarXiv:1508.04025,2015.\n11\n\n[PAGE 12]\n[25] MitchellPMarcus,MaryAnnMarcinkiewicz,andBeatriceSantorini.Buildingalargeannotated\ncorpusofenglish: Thepenntreebank. Computationallinguistics,19(2):313–330,1993.\n[26] DavidMcClosky,EugeneCharniak,andMarkJohnson. Effectiveself-trainingforparsing. In\nProceedingsoftheHumanLanguageTechnologyConferenceoftheNAACL,MainConference,\npages152–159.ACL,June2006.\n[27] AnkurParikh,OscarTäckström,DipanjanDas,andJakobUszkoreit. Adecomposableattention\nmodel. InEmpiricalMethodsinNaturalLanguageProcessing,2016.\n[28] RomainPaulus,CaimingXiong,andRichardSocher. Adeepreinforcedmodelforabstractive\nsummarization. arXivpreprintarXiv:1705.04304,2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputationalLinguisticsand44thAnnualMeetingoftheACL,pages433–440.ACL,July\n2006.\n[30] OfirPressandLiorWolf. Usingtheoutputembeddingtoimprovelanguagemodels. arXiv\npreprintarXiv:1608.05859,2016.\n[31] RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrarewords\nwithsubwordunits. arXivpreprintarXiv:1508.07909,2015.\n[32] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,\nandJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-experts\nlayer. arXivpreprintarXiv:1701.06538,2017.\n[33] NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi-\nnov. Dropout: asimplewaytopreventneuralnetworksfromoverfitting. JournalofMachine\nLearningResearch,15(1):1929–1958,2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. InC.Cortes, N.D.Lawrence, D.D.Lee, M.Sugiyama, andR.Garnett, editors,\nAdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates,\nInc.,2015.\n[35] IlyaSutskever,OriolVinyals,andQuocVVLe. Sequencetosequencelearningwithneural\nnetworks. InAdvancesinNeuralInformationProcessingSystems,pages3104–3112,2014.\n[36] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna.\nRethinkingtheinceptionarchitectureforcomputervision. CoRR,abs/1512.00567,2015.\n[37] Vinyals&Kaiser, Koo, Petrov, Sutskever, andHinton. Grammarasaforeignlanguage. In\nAdvancesinNeuralInformationProcessingSystems,2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal. Google’sneuralmachine\ntranslationsystem: Bridgingthegapbetweenhumanandmachinetranslation. arXivpreprint\narXiv:1609.08144,2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep r"
      },
      {
        "id": 1,
        "score": 0.2107725739479065,
        "chunk": "allyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstru"
      },
      {
        "id": 9,
        "score": 0.20983263850212097,
        "chunk": "anau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointly\nlearningtoalignandtranslate. CoRR,abs/1409.0473,2014.\n[3] DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le. Massiveexplorationofneural\nmachinetranslationarchitectures. CoRR,abs/1703.03906,2017.\n[4] JianpengCheng,LiDong,andMirellaLapata. Longshort-termmemory-networksformachine\nreading. arXivpreprintarXiv:1601.06733,2016.\n10\n\n[PAGE 11]\n[5] KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk,\nandYoshuaBengio. Learningphraserepresentationsusingrnnencoder-decoderforstatistical\nmachinetranslation. CoRR,abs/1406.1078,2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprintarXiv:1610.02357,2016.\n[7] JunyoungChung,ÇaglarGülçehre,KyunghyunCho,andYoshuaBengio. Empiricalevaluation\nofgatedrecurrentneuralnetworksonsequencemodeling. CoRR,abs/1412.3555,2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetworkgrammars. InProc.ofNAACL,2016.\n[9] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolu-\ntionalsequencetosequencelearning. arXivpreprintarXiv:1705.03122v2,2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850,2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition,pages770–778,2016.\n[12] SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber. Gradientflowin\nrecurrentnets: thedifficultyoflearninglong-termdependencies,2001.\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735–1780,1997.\n[14] ZhongqiangHuangandMaryHarper. Self-trainingPCFGgrammarswithlatentannotations\nacrosslanguages. InProceedingsofthe2009ConferenceonEmpiricalMethodsinNatural\nLanguageProcessing,pages832–841.ACL,August2009.\n[15] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. Exploring\nthelimitsoflanguagemodeling. arXivpreprintarXiv:1602.02410,2016.\n[16] ŁukaszKaiserandSamyBengio. Canactivememoryreplaceattention? InAdvancesinNeural\nInformationProcessingSystems,(NIPS),2016.\n[17] ŁukaszKaiserandIlyaSutskever. NeuralGPUslearnalgorithms. InInternationalConference\nonLearningRepresentations(ICLR),2016.\n[18] NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo-\nrayKavukcuoglu.Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2,\n2017.\n[19] YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush. Structuredattentionnetworks.\nInInternationalConferenceonLearningRepresentations,2017.\n[20] DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InICLR,2015.\n[21] OleksiiKuchaievandBorisGinsburg. FactorizationtricksforLSTMnetworks. arXivpreprint\narXiv:1703.10722,2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-att"
      }
    ],
    "non_rag_answer": "the re-evaluation of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the underlying principles of the "
  }
]